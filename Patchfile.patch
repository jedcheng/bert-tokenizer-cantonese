diff --git a/add_cantonese_tokens.py b/add_cantonese_tokens.py
index 955fe76..d4b19ee 100644
--- a/add_cantonese_tokens.py
+++ b/add_cantonese_tokens.py
@@ -1,55 +1,57 @@
-from collections import Counter
-from random import Random
-from transformers import BertTokenizerFast
+def add_cantonese_tokens():
 
-from lib import load_lihkg, token_to_token_id as vocab_old, is_unused, is_cjkv
+    from collections import Counter
+    from random import Random
+    from transformers import BertTokenizerFast
 
-rng = Random(42)
-sentences = load_lihkg()
-sentences = rng.choices(sentences, k=524288)
+    from lib import load_lihkg, token_to_token_id as vocab_old, is_unused, is_cjkv
 
-tokenizer_old = BertTokenizerFast.from_pretrained('ckiplab/gpt2-tiny-chinese')
-# tokenizer_new = tokenizer_old.train_new_from_iterator(sentences, 2048, length=len(sentences))
+    rng = Random(42)
+    sentences = load_lihkg()
+    sentences = rng.choices(sentences, k=524288)
 
-########
+    tokenizer_old = BertTokenizerFast.from_pretrained('ckiplab/gpt2-tiny-chinese')
+    # tokenizer_new = tokenizer_old.train_new_from_iterator(sentences, 2048, length=len(sentences))
 
-vocab_new = set()
+    ########
 
-with open('vocab_mapping.txt', encoding='utf-8') as f:
-    for line in f:
-        token, token_id = line.rstrip('\n').rsplit(' ', 1)
-        if is_unused(token):
-            continue
-        vocab_new.add(token)
+    vocab_new = set()
 
-########
+    with open('vocab_mapping.txt', encoding='utf-8') as f:
+        for line in f:
+            token, token_id = line.rstrip('\n').rsplit(' ', 1)
+            if is_unused(token):
+                continue
+            vocab_new.add(token)
 
-counter = Counter()
+    ########
 
-for sentence in sentences:
-    for c in sentence:
-        if is_cjkv(c) and c not in vocab_new:
-            counter[c] += 1
+    counter = Counter()
+
+    for sentence in sentences:
+        for c in sentence:
+            if is_cjkv(c) and c not in vocab_new:
+                counter[c] += 1
 
-cjkv_new = set((token for token, _ in counter.most_common(150)))
+    cjkv_new = set((token for token, _ in counter.most_common(150)))
 
-########
+    ########
 
-with open('yue.txt', encoding='utf-8') as f:
-    text = f.read()
+    with open('yue.txt', encoding='utf-8') as f:
+        text = f.read()
 
-for c in text:
-    if is_cjkv(c) and c not in vocab_new:
-        cjkv_new.add(c)
+    for c in text:
+        if is_cjkv(c) and c not in vocab_new:
+            cjkv_new.add(c)
 
-########
+    ########
 
-cs = set()
-for c in cjkv_new:
-    if c not in vocab_old:
-        cs.add(c)
+    cs = set()
+    for c in cjkv_new:
+        if c not in vocab_old:
+            cs.add(c)
 
-########
+    ########
 
-with open('add_token.txt', 'w', encoding='utf-8') as f:
-    print(''.join(sorted(cs)), file=f)
+    with open('add_token.txt', 'w', encoding='utf-8') as f:
+        print(''.join(sorted(cs)), file=f)
diff --git a/build_vocab_mapping.py b/build_vocab_mapping.py
index 0e24cfa..8ea8ec6 100644
--- a/build_vocab_mapping.py
+++ b/build_vocab_mapping.py
@@ -1,109 +1,111 @@
-from collections import defaultdict
-from transformers import BertTokenizer
+def build_vocab_mapping():
+    from collections import defaultdict
+    from transformers import BertTokenizer
 
-tokenizer = BertTokenizer.from_pretrained('ckiplab/gpt2-tiny-chinese')
-tokenizer.save_vocabulary('vocab-bart-base-chinese.txt')
+    tokenizer = BertTokenizer.from_pretrained('ckiplab/gpt2-tiny-chinese')
+    tokenizer.save_vocabulary('vocab-bart-base-chinese.txt')
 
-from lib import token_id_to_token, token_to_token_id, conv_table, is_alpha_char, is_cjkv
+    from lib import token_id_to_token, token_to_token_id, conv_table, is_alpha_char, is_cjkv
+    from typing import List, Set
 
-###########
+    ###########
 
-token_new_to_candidate_token_ids = defaultdict(set)
+    token_new_to_candidate_token_ids = defaultdict(set)
 
-for token_id, token in token_id_to_token.items():
-    if not is_cjkv(token):  # non-CJKV
-        token_new_to_candidate_token_ids[token].add(token_id)
+    for token_id, token in token_id_to_token.items():
+        if not is_cjkv(token):  # non-CJKV
+            token_new_to_candidate_token_ids[token].add(token_id)
 
-    else:  # CJKV
-        trads = conv_table.get(token)
+        else:  # CJKV
+            trads = conv_table.get(token)
 
-        if trads is None:
-            token_new_to_candidate_token_ids[token].add(token_id)
-        elif len(trads) == 1:
-            trad = trads
-            token_new_to_candidate_token_ids[trad].add(token_id)
-        else:
-            trad_first, *trad_rests = trads
+            if trads is None:
+                token_new_to_candidate_token_ids[token].add(token_id)
+            elif len(trads) == 1:
+                trad = trads
+                token_new_to_candidate_token_ids[trad].add(token_id)
+            else:
+                trad_first, *trad_rests = trads
 
-            # trad_first
-            token_new_to_candidate_token_ids[trad_first].add(token_id)
+                # trad_first
+                token_new_to_candidate_token_ids[trad_first].add(token_id)
 
-            # trad_rests
-            for trad_rest in trad_rests:
-                if trad_rest in token_to_token_id:
-                    token_id_new = token_to_token_id[trad_rest]
-                else:
-                    token_id_new = token_id
-                token_new_to_candidate_token_ids[trad_rest].add(token_id_new)
+                # trad_rests
+                for trad_rest in trad_rests:
+                    if trad_rest in token_to_token_id:
+                        token_id_new = token_to_token_id[trad_rest]
+                    else:
+                        token_id_new = token_id
+                    token_new_to_candidate_token_ids[trad_rest].add(token_id_new)
 
-###########
+    ###########
 
-def filter_candidate_token_ids(token_new: str, candidate_token_ids: set[int]) -> set[int]:
-    # non-CJKV tokens
+    def filter_candidate_token_ids(token_new: str, candidate_token_ids: Set[int]) -> Set[int]:
+        # non-CJKV tokens
 
-    if not is_cjkv(token_new):
-        return candidate_token_ids
+        if not is_cjkv(token_new):
+            return candidate_token_ids
 
-    # CJKV tokens with length of 1
+        # CJKV tokens with length of 1
 
-    if len(candidate_token_ids) == 1:
-        return candidate_token_ids
+        if len(candidate_token_ids) == 1:
+            return candidate_token_ids
 
-    # CJKV tokens with length greater than 1
+        # CJKV tokens with length greater than 1
 
-    candidate_token_ids_new = set()
+        candidate_token_ids_new = set()
 
-    for candidate_token_id in candidate_token_ids:
-        candidate_token = token_id_to_token[candidate_token_id]
+        for candidate_token_id in candidate_token_ids:
+            candidate_token = token_id_to_token[candidate_token_id]
 
-        if not is_alpha_char(token_new) and candidate_token == token_new:
-            continue
+            if not is_alpha_char(token_new) and candidate_token == token_new:
+                continue
 
-        candidate_token_ids_new.add(candidate_token_id)
+            candidate_token_ids_new.add(candidate_token_id)
 
-    return candidate_token_ids_new
+        return candidate_token_ids_new
 
-token_new_to_candidate_token_ids = {
-    token_new: filter_candidate_token_ids(token_new, candidate_token_ids)
-    for token_new, candidate_token_ids
-    in token_new_to_candidate_token_ids.items()
-}
+    token_new_to_candidate_token_ids = {
+        token_new: filter_candidate_token_ids(token_new, candidate_token_ids)
+        for token_new, candidate_token_ids
+        in token_new_to_candidate_token_ids.items()
+    }
 
-###########
+    ###########
 
-token_new_to_token_id_new = []
+    token_new_to_token_id_new = []
 
-for token_new, candidate_token_ids in token_new_to_candidate_token_ids.items():
-    if len(candidate_token_ids) == 1:
-        token_id_new = next(iter(candidate_token_ids))
+    for token_new, candidate_token_ids in token_new_to_candidate_token_ids.items():
+        if len(candidate_token_ids) == 1:
+            token_id_new = next(iter(candidate_token_ids))
 
-    elif len(candidate_token_ids) > 1:
-        candidate_tokens = [
-            token_id_to_token[candidate_token_id]
-            for candidate_token_id
-            in candidate_token_ids
-        ]
+        elif len(candidate_token_ids) > 1:
+            candidate_tokens = [
+                token_id_to_token[candidate_token_id]
+                for candidate_token_id
+                in candidate_token_ids
+            ]
 
-        # print(token_new, ''.join(candidate_tokens))  # for generating `preferences`
+            # print(token_new, ''.join(candidate_tokens))  # for generating `preferences`
 
-        preferences = {
-            '麼': '么',  # 么麽
-            '於': '于',  # 於于
-            '夥': '伙',  # 伙夥
-            '餘': '余',  # 余馀
-            '徵': '征',  # 徵征
-            '鍾': '钟',  # 钟锺
-            '諮': '咨',  # 咨谘
-            '麪': '麺',  # 麺面
-        }
-        candidate_token = preferences[token_new]  # guaranteed that `token_new` is always inside `preferences`
-        token_id_new = token_to_token_id[candidate_token]
+            preferences = {
+                '麼': '么',  # 么麽
+                '於': '于',  # 於于
+                '夥': '伙',  # 伙夥
+                '餘': '余',  # 余馀
+                '徵': '征',  # 徵征
+                '鍾': '钟',  # 钟锺
+                '諮': '咨',  # 咨谘
+                '麪': '麺',  # 麺面
+            }
+            candidate_token = preferences[token_new]  # guaranteed that `token_new` is always inside `preferences`
+            token_id_new = token_to_token_id[candidate_token]
 
-    else:  # len(candidate_token_ids) == 0
-        raise ValueError('The length of `candidate_token_ids` should not be zero.')
+        else:  # len(candidate_token_ids) == 0
+            raise ValueError('The length of `candidate_token_ids` should not be zero.')
 
-    token_new_to_token_id_new.append((token_new, token_id_new))
+        token_new_to_token_id_new.append((token_new, token_id_new))
 
-with open('vocab_mapping.txt', 'w', encoding='utf-8') as f:
-    for token_new, token_id_new in token_new_to_token_id_new:
-        print(token_new, token_id_new, file=f)
+    with open('vocab_mapping.txt', 'w', encoding='utf-8') as f:
+        for token_new, token_id_new in token_new_to_token_id_new:
+            print(token_new, token_id_new, file=f)
diff --git a/lib/load_lihkg.py b/lib/load_lihkg.py
index 4fe9454..2d465a2 100644
--- a/lib/load_lihkg.py
+++ b/lib/load_lihkg.py
@@ -1,6 +1,7 @@
 from os.path import expanduser
+from typing import List
 
-def load_lihkg() -> list[str]:
+def load_lihkg() -> List[str]:
     filename = expanduser('lihkg-1-2850000-processed-dedup.csv')
     sentences = []
     with open(filename, encoding='utf-8') as f:
diff --git a/replace_embedding.py b/replace_embedding.py
index 8f083ff..77a33cf 100644
--- a/replace_embedding.py
+++ b/replace_embedding.py
@@ -1,80 +1,82 @@
-import jax; jax.config.update('jax_platforms', 'cpu')
+def replace_embedding():
 
-import jax.numpy as np
-from transformers import BertTokenizer, FlaxBartModel
+    import jax; jax.config.update('jax_platforms', 'cpu')
 
-from lib import random_init_embed, save_params, seed2key
+    import jax.numpy as np
+    from transformers import BertTokenizer, FlaxBartModel
 
-tokenizer = BertTokenizer.from_pretrained('ckiplab/gpt2-tiny-chinese')
-model = FlaxBartModel.from_pretrained('ckiplab/gpt2-tiny-chinesee', from_pt=True)
+    from lib import random_init_embed, save_params, seed2key
 
-key = seed2key(42)
+    tokenizer = BertTokenizer.from_pretrained('ckiplab/gpt2-tiny-chinese')
+    model = FlaxBartModel.from_pretrained('ckiplab/gpt2-tiny-chinese', from_pt=True)
 
-##########
+    key = seed2key(42)
 
-token_new_to_token_id = {}
+    ##########
 
-with open('vocab_mapping.txt', encoding='utf-8') as f:
-    for line in f:
-        token_new, token_id = line.rstrip('\n').rsplit(' ', 1)
-        token_id = int(token_id)
-        token_new_to_token_id[token_new] = token_id
+    token_new_to_token_id = {}
 
-##########
+    with open('vocab_mapping.txt', encoding='utf-8') as f:
+        for line in f:
+            token_new, token_id = line.rstrip('\n').rsplit(' ', 1)
+            token_id = int(token_id)
+            token_new_to_token_id[token_new] = token_id
 
-with open('add_token.txt', encoding='utf-8') as f:
-    new_chars = f.read().rstrip('\n')
+    ##########
 
-##########
+    with open('add_token.txt', encoding='utf-8') as f:
+        new_chars = f.read().rstrip('\n')
 
-num_of_unused = sum(1 for token in token_new_to_token_id if token.startswith('[unused'))
-assert num_of_unused == 99
+    ##########
 
-new_chars_a = new_chars[:num_of_unused]
-new_chars_b = new_chars[num_of_unused:]
+    num_of_unused = sum(1 for token in token_new_to_token_id if token.startswith('[unused'))
+    assert num_of_unused == 99
 
-##########
+    new_chars_a = new_chars[:num_of_unused]
+    new_chars_b = new_chars[num_of_unused:]
 
-emb_old = list(model.params['shared']['embedding'])
-emb_new = list(random_init_embed(key, len(new_chars_b)))
+    ##########
 
-##########
+    emb_old = list(model.params['shared']['embedding'])
+    emb_new = list(random_init_embed(key, len(new_chars_b)))
 
-token_new_to_emb = {}
+    ##########
 
-unused_idx = 1
-new_chars_iter = iter(new_chars_a)
+    token_new_to_emb = {}
 
-for token_new, token_id in token_new_to_token_id.items():
-    if token_new.startswith('[unused'):
-        token_id = token_new_to_token_id[f'[unused{unused_idx}]']
-        new_char = next(new_chars_iter)
-        token_new_to_emb[new_char] = emb_old[token_id]
-        unused_idx += 1
-    else:
-        token_new_to_emb[token_new] = emb_old[token_id]
+    unused_idx = 1
+    new_chars_iter = iter(new_chars_a)
 
-for i, token_new in enumerate(new_chars_b):
-    token_new_to_emb[token_new] = emb_new[i]
+    for token_new, token_id in token_new_to_token_id.items():
+        if token_new.startswith('[unused'):
+            token_id = token_new_to_token_id[f'[unused{unused_idx}]']
+            new_char = next(new_chars_iter)
+            token_new_to_emb[new_char] = emb_old[token_id]
+            unused_idx += 1
+        else:
+            token_new_to_emb[token_new] = emb_old[token_id]
 
-##########
+    for i, token_new in enumerate(new_chars_b):
+        token_new_to_emb[token_new] = emb_new[i]
 
-vocab = []
-emb_all = []
+    ##########
 
-for i, (token_new, emb) in enumerate(token_new_to_emb.items()):
-    assert isinstance(token_new, str)
-    assert emb.shape[0] == 768
-    assert token_new != '\n'
-    vocab.append(token_new)
-    emb_all.append(emb)
+    vocab = []
+    emb_all = []
 
-emb_all = np.vstack(emb_all)
+    for i, (token_new, emb) in enumerate(token_new_to_emb.items()):
+        assert isinstance(token_new, str)
+        assert emb.shape[0] == 768
+        assert token_new != '\n'
+        vocab.append(token_new)
+        emb_all.append(emb)
 
-##########
+    emb_all = np.vstack(emb_all)
 
-with open('vocab-bart-base-cantonese.txt', 'w', encoding='utf-8') as f:
-    for token in vocab:
-        print(token, file=f)
+    ##########
 
-save_params(emb_all, 'embed_params.dat')
+    with open('vocab-bart-base-cantonese.txt', 'w', encoding='utf-8') as f:
+        for token in vocab:
+            print(token, file=f)
+
+    save_params(emb_all, 'embed_params.dat')
